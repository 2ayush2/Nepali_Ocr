{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from math import atan2, cos, sin, sqrt, pi\n",
    "import logging  # For better debugging\n",
    "from typing import Optional, Tuple, List  # For type hints\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "import pytesseract\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Set Tesseract path (update this based on your installation)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Windows example\n",
    "\n",
    "# Load pre-trained Faster R-CNN model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define input directory\n",
    "input_folder = Path(\"input\")\n",
    "input_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Function to preprocess image for Faster R-CNN\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(str(image_path))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = F.to_tensor(img_rgb).unsqueeze(0)  # Add batch dimension\n",
    "    return img, img_rgb, img_tensor\n",
    "\n",
    "# Function to enhance image (deblur, contrast adjustment, glare removal)\n",
    "def enhance_image(image):\n",
    "    # Deblur using Gaussian blur and sharpening\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    sharpened = cv2.addWeighted(image, 1.5, blurred, -0.5, 0)\n",
    "    \n",
    "    # Contrast adjustment using CLAHE\n",
    "    lab = cv2.cvtColor(sharpened, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    l = clahe.apply(l)\n",
    "    lab = cv2.merge((l, a, b))\n",
    "    enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    # Glare removal\n",
    "    gray = cv2.cvtColor(enhanced, cv2.COLOR_BGR2GRAY)\n",
    "    _, mask = cv2.threshold(gray, 220, 255, cv2.THRESH_BINARY)\n",
    "    enhanced = cv2.inpaint(enhanced, mask, 3, cv2.INPAINT_TELEA)\n",
    "    return enhanced\n",
    "\n",
    "# Function to correct skew and align the ID card\n",
    "def correct_skew(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi / 180, 200)\n",
    "    angle = 0\n",
    "    if lines is not None:\n",
    "        for rho, theta in lines[0]:\n",
    "            angle = (theta * 180 / np.pi) - 90\n",
    "            break\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h))\n",
    "    return rotated\n",
    "\n",
    "# Function to detect ROIs with Faster R-CNN\n",
    "def detect_rois(model, img_tensor, threshold=0.5):\n",
    "    with torch.no_grad():\n",
    "        predictions = model(img_tensor)[0]\n",
    "    boxes = predictions['boxes'].cpu().numpy()\n",
    "    scores = predictions['scores'].cpu().numpy()\n",
    "    labels = predictions['labels'].cpu().numpy()\n",
    "    mask = scores > threshold\n",
    "    return boxes[mask], scores[mask], labels[mask]\n",
    "\n",
    "# Function to extract text from ROIs using OCR with preprocessing\n",
    "def extract_text_from_roi(image, box):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    roi = image[y1:y2, x1:x2]\n",
    "    # Preprocess ROI for better OCR\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    text = pytesseract.image_to_string(thresh, config='--psm 6')\n",
    "    return text.strip()\n",
    "\n",
    "# Function to crop the ID card from the image\n",
    "def crop_id_card(image, boxes):\n",
    "    if len(boxes) == 0:\n",
    "        return image\n",
    "    largest_box = max(boxes, key=lambda b: (b[2] - b[0]) * (b[3] - b[1]))\n",
    "    x1, y1, x2, y2 = map(int, largest_box)\n",
    "    cropped = image[y1:y2, x1:x2]\n",
    "    return cropped\n",
    "\n",
    "# Function to find a suitable text placement region (e.g., bottom of the ID card)\n",
    "def find_text_placement_region(image):\n",
    "    h, w = image.shape[:2]\n",
    "    # Assume bottom 20% of the image is a good spot for text (adjustable)\n",
    "    return h - int(h * 0.2), h, 0, w\n",
    "\n",
    "# Main processing function to display enhanced ID card with naturally integrated OCR text\n",
    "def process_and_display_id_card(image_path):\n",
    "    # Load and preprocess image\n",
    "    original_img, img_rgb, img_tensor = preprocess_image(image_path)\n",
    "\n",
    "    # Enhance and align the image\n",
    "    enhanced_img = enhance_image(original_img)\n",
    "    enhanced_img = correct_skew(enhanced_img)\n",
    "\n",
    "    # Detect ROIs\n",
    "    _, enhanced_rgb, enhanced_tensor = preprocess_image(image_path)\n",
    "    boxes, scores, labels = detect_rois(model, enhanced_tensor)\n",
    "\n",
    "    # Extract text from ROIs\n",
    "    ocr_results = []\n",
    "    for i, box in enumerate(boxes[:5]):  # Limit to top 5 ROIs\n",
    "        text = extract_text_from_roi(enhanced_img, box)\n",
    "        confidence = scores[i] * 100\n",
    "        if text:\n",
    "            ocr_results.append(f\"{text} ({confidence:.1f}%)\")\n",
    "\n",
    "    # Crop the ID card\n",
    "    cropped_id = crop_id_card(enhanced_img, boxes)\n",
    "    cropped_id = enhance_image(cropped_id)  # Re-enhance for best quality\n",
    "\n",
    "    # Find placement region for text (bottom of the ID card)\n",
    "    y_start, y_end, x_start, x_end = find_text_placement_region(cropped_id)\n",
    "    output_img = cropped_id.copy()\n",
    "\n",
    "    # Overlay OCR text naturally on the ID card\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.7\n",
    "    font_thickness = 2\n",
    "    text_color = (0, 0, 0)  # Black text for readability\n",
    "    bg_color = (255, 255, 255, 50)  # Semi-transparent white background\n",
    "\n",
    "    y_offset = y_start + 20\n",
    "    for result in ocr_results:\n",
    "        # Calculate text size\n",
    "        (text_w, text_h), baseline = cv2.getTextSize(result, font, font_scale, font_thickness)\n",
    "        \n",
    "        # Draw a semi-transparent background rectangle for contrast\n",
    "        bg_x, bg_y = 10, y_offset - text_h - 5\n",
    "        bg_w, bg_h = text_w + 10, text_h + 10\n",
    "        overlay = output_img.copy()\n",
    "        cv2.rectangle(overlay, (bg_x, bg_y), (bg_x + bg_w, bg_y + bg_h), bg_color[:3], -1)\n",
    "        cv2.addWeighted(overlay, 0.7, output_img, 0.3, 0, output_img)\n",
    "        \n",
    "        # Add text\n",
    "        cv2.putText(output_img, result, (bg_x + 5, y_offset), font, font_scale, text_color, font_thickness)\n",
    "        y_offset += text_h + 15\n",
    "\n",
    "        # Stop if we exceed the image height\n",
    "        if y_offset + text_h > output_img.shape[0]:\n",
    "            break\n",
    "\n",
    "    # Display the result\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Enhanced ID Card with Natural OCR Text\")\n",
    "    plt.show()\n",
    "\n",
    "# Process the first image in the input folder\n",
    "image_files = list(input_folder.glob(\"*.jpg\"))\n",
    "if image_files:\n",
    "    process_and_display_id_card(image_files[0])\n",
    "else:\n",
    "    print(\"No images found in the 'input' folder. Please add an ID card image (e.g., 'id_card.jpg').\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
